{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T14:35:27.523845Z",
     "start_time": "2018-02-21T14:35:25.088889Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import ast\n",
    "\n",
    "import IPython.display as ipd\n",
    "from tqdm import tqdm_notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import keras\n",
    "from keras.layers import Activation, Dense, Conv1D, Conv2D, MaxPooling1D, Flatten, Reshape, BatchNormalization, Dropout\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder, StandardScaler, LabelBinarizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "import utils\n",
    "from utils import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T14:35:27.531791Z",
     "start_time": "2018-02-21T14:35:27.523845Z"
    }
   },
   "outputs": [],
   "source": [
    "AUDIO_DIR = \"..\\\\fma_small\"\n",
    "META_DIR = \"..\\\\fma_metadata\"\n",
    "SUBSET = 'small'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T14:35:27.667781Z",
     "start_time": "2018-02-21T14:35:27.655772Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load metadata to memory\n",
    "def load_meta_data(): \n",
    "    tracks_all   = utils.load(META_DIR + '\\\\tracks.csv')\n",
    "    features_all = utils.load(META_DIR + '\\\\features.csv')\n",
    "    echonest_all = utils.load(META_DIR + '\\\\echonest.csv')\n",
    "\n",
    "    #genres = utils.load(META_DIR + 'genres.csv')\n",
    "\n",
    "    np.testing.assert_array_equal(features_all.index, tracks_all.index)\n",
    "    assert echonest_all.index.isin(tracks_all.index).all()\n",
    "    \n",
    "    \n",
    "    return tracks_all, features_all, echonest_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T14:35:28.155765Z",
     "start_time": "2018-02-21T14:35:28.147765Z"
    }
   },
   "outputs": [],
   "source": [
    "# Choose Subset\n",
    "def choose_small_subset(tracks_all, features_all, echonest_all):\n",
    "    subset = tracks_all.index[tracks_all['set', 'subset'] <= 'small']\n",
    "\n",
    "    assert subset.isin(tracks_all.index).all()\n",
    "    assert subset.isin(features_all.index).all()\n",
    "    \n",
    "    tracks = tracks_all.loc[subset]\n",
    "    features = features_all.loc[subset]\n",
    "\n",
    "    return tracks, features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T14:35:54.113433Z",
     "start_time": "2018-02-21T14:35:28.825309Z"
    }
   },
   "outputs": [],
   "source": [
    "tracks_all, features_all, echonest_all = load_meta_data()\n",
    "tracks, features =  choose_small_subset(tracks_all, features_all, echonest_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T14:35:54.121372Z",
     "start_time": "2018-02-21T14:35:54.113433Z"
    }
   },
   "outputs": [],
   "source": [
    "tracks.shape, features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train Val Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T14:35:54.185343Z",
     "start_time": "2018-02-21T14:35:54.121372Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Splitting into Train, Validation, Test\n",
    "train_index = tracks.index[tracks['set', 'split'] == 'training']\n",
    "val_index   = tracks.index[tracks['set', 'split'] == 'validation']\n",
    "test_index  = tracks.index[tracks['set', 'split'] == 'test']\n",
    "\n",
    "\n",
    "print('{} training examples'.format(len(train_index)))\n",
    "print('{} validation examples'.format(len(val_index)))\n",
    "print('{} testing examples'.format(len(test_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T14:35:54.201375Z",
     "start_time": "2018-02-21T14:35:54.185343Z"
    }
   },
   "outputs": [],
   "source": [
    "X = features.values\n",
    "Y = tracks['track']['genre_top']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T14:35:54.273342Z",
     "start_time": "2018-02-21T14:35:54.201375Z"
    }
   },
   "outputs": [],
   "source": [
    "Xtrain = features.loc[train_index].values\n",
    "Xval  = features.loc[val_index].values\n",
    "Xtest  = features.loc[test_index].values\n",
    "\n",
    "Ytrain = tracks.loc[train_index]['track']['genre_top'].values\n",
    "Yval = tracks.loc[val_index]['track']['genre_top'].values\n",
    "Ytest = tracks.loc[test_index]['track']['genre_top'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T14:35:54.285345Z",
     "start_time": "2018-02-21T14:35:54.277346Z"
    }
   },
   "outputs": [],
   "source": [
    "classes = list(set(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Model Using Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T14:42:48.191075Z",
     "start_time": "2018-02-21T14:42:48.171048Z"
    }
   },
   "outputs": [],
   "source": [
    "trainIDs = tracks.loc[train_index][\"track\"].index.values\n",
    "valIDs  = tracks.loc[val_index][\"track\"].index.values\n",
    "testIDs  = tracks.loc[test_index][\"track\"].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T14:42:55.007607Z",
     "start_time": "2018-02-21T14:42:54.983575Z"
    }
   },
   "outputs": [],
   "source": [
    "labels_onehot = MultiLabelBinarizer().fit_transform(tracks['track', 'genre_top'])\n",
    "labels_onehot = pd.DataFrame(labels_onehot, index=tracks.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN using the 128x128 patches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPEC_DIR = \"..\\\\spectrogram\\\\\"\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, GRU\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "def cnn_model(input_shape=(105,105,3), output = 8):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(BatchNormalization(axis=2, input_shape=input_shape))\n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(Activation('elu'))    \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.1))\n",
    "    \n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(BatchNormalization(axis=3))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(BatchNormalization(axis=3))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.1))\n",
    "#     model.summary()\n",
    "#     model.add(Flatten())\n",
    "    model.add(Reshape((11*11, 64)))\n",
    "    model.add(GRU(32, return_sequences=True, name='gru1'))\n",
    "    model.add(GRU(32, return_sequences=False, name='gru2'))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(output))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',  \n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "    \n",
    "# cnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        SPEC_DIR + 'train',  \n",
    "        \n",
    "        target_size=(105, 105),\n",
    "        batch_size=batch_size)\n",
    "\n",
    "val_generator = test_datagen.flow_from_directory(\n",
    "        SPEC_DIR + 'val',\n",
    "        target_size=(105, 105),\n",
    "        batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = cnn_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFBoard = init_env_and_tfboard(\"cnn\")\n",
    "model = cnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('crnn_try_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "#         \"callbacks\": [TFBoard],\n",
    "        \"verbose\": 2\n",
    "}\n",
    "\n",
    "model.fit_generator(train_generator, \n",
    "                    steps_per_epoch = 10000 / batch_size, \n",
    "                    epochs = 30,\n",
    "                    validation_data=val_generator,\n",
    "                    validation_steps=800 // batch_size,\n",
    "                    **params)\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('crnn_try_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
